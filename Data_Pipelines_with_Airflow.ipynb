{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install apache-airflow\n"
      ],
      "metadata": {
        "id": "X1mYvUqNES10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "jpuz_9tBD_OU",
        "outputId": "f82ac73d-11e3-4d77-c32b-57ddc40c2e4d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m<\u001b[0m\u001b[1;33mipython-input-\u001b[0m\u001b[1;33m2\u001b[0m\u001b[1;33m-aaeea2563f10\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m2\u001b[0m\u001b[1;33m DeprecationWarning\u001b[0m\u001b[33m: The `airflow.operators.python_operator.PythonOperator` class is deprecated. Please use `\u001b[0m\u001b[33m'airflow.operators.python.PythonOperator'\u001b[0m\u001b[33m`.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">&lt;ipython-input-</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">-aaeea2563f10&gt;:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> DeprecationWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: The `airflow.operators.python_operator.PythonOperator` class is deprecated. Please use `</span><span style=\"color: #808000; text-decoration-color: #808000\">'airflow.operators.python.PythonOperator'</span><span style=\"color: #808000; text-decoration-color: #808000\">`.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\u001b[34m2023-07-02T05:18:26.385+0000\u001b[0m] {\u001b[34mutils.py:\u001b[0m160} INFO\u001b[0m - NumExpr defaulting to 2 threads.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "default_args = {\n",
        "    'owner': 'MTN Rwanda',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2023, 4, 18),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 2,\n",
        "    'retry_delay': timedelta(minutes=5)\n",
        "}\n",
        "\n",
        "dag = DAG('data_pipeline', default_args=default_args, schedule_interval='@daily')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "H6scQKZTE9JM",
        "outputId": "a07d7cb5-a675-41af-d354-ee9cb8df17cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m<\u001b[0m\u001b[1;33mipython-input-\u001b[0m\u001b[1;33m3\u001b[0m\u001b[1;33m-d866b10183a6\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m11\u001b[0m\u001b[1;33m RemovedInAirflow3Warning\u001b[0m\u001b[33m: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">&lt;ipython-input-</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">-d866b10183a6&gt;:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">11</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RemovedInAirflow3Warning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data():\n",
        "    # extract data from CSV files\n",
        "    customer_df = pd.read_csv('customer_data.csv')\n",
        "    order_df = pd.read_csv('order_data.csv')\n",
        "    payment_df = pd.read_csv('payment_data.csv')\n",
        "\n",
        "    # load the CSV data into Pandas dataframes for later transformation\n",
        "    return customer_df, order_df, payment_df"
      ],
      "metadata": {
        "id": "h8mRvKmCE_bN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_data(customer_df, order_df, payment_df):\n",
        "\n",
        "    #DATA TRANSFORMATION\n",
        "    customer_df['date_of_birth'] = pd.to_datetime(customer_df['date_of_birth'])\n",
        "    order_df['order_date'] = pd.to_datetime(order_df['order_date'])\n",
        "    payment_df['payment_date'] = pd.to_datetime(payment_df['payment_date'])\n",
        "\n",
        "    customer_order_data = pd.merge(customer_df, order_df, on='customer_id')\n",
        "    payment_customer_order_data = pd.merge(payment_df, customer_order_data, on=['order_id', 'customer_id'])\n",
        "\n",
        "    payment_customer_order_data.drop(['customer_id', 'order_id'], axis=1, inplace=True)\n",
        "\n",
        "    customer_lifetime_value = payment_customer_order_data.groupby(['email', 'country', 'gender', 'date_of_birth']).agg(total_amount_paid=('amount', 'sum'), number_of_orders=('product', 'count')).reset_index()\n",
        "\n",
        "    customer_lifetime_value['total_order_value'] = customer_lifetime_value['total_amount_paid'] / customer_lifetime_value['number_of_orders']\n",
        "\n",
        "    customer_lifetime_value['lifespan'] = (pd.Timestamp.today() - customer_lifetime_value['date_of_birth']).dt.days / 365.25\n",
        "    customer_lifetime_value['average_order_value'] = customer_lifetime_value['total_order_value']\n",
        "    customer_lifetime_value['clv'] = customer_lifetime_value['average_order_value'] * customer_lifetime_value['number_of_orders'] * customer_lifetime_value['lifespan']\n",
        "\n",
        "    return customer_lifetime_value"
      ],
      "metadata": {
        "id": "vp-BNsy6GGNM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wrapping the code that loads the data into a try/except block\n",
        "def load_data(transformed_data):\n",
        "    try:\n",
        "\n",
        "#To load the transformed data into a Postgres database, you can use the create_engine function\n",
        "        engine = create_engine('postgresql://username:password@localhost:5432/dbname')\n",
        "        connection = engine.connect()\n",
        "        table_name = 'subscriber_data'\n",
        "\n",
        "        # create table if it doesn't exist\n",
        "        if not engine.has_table(table_name):\n",
        "            transformed_data.iloc[0:0].to_sql(table_name, engine, if_exists='replace', index=False)\n",
        "\n",
        "        # insert data into table\n",
        "        transformed_data.to_sql(table_name, connection, if_exists='append', index=False)\n",
        "\n",
        "        connection.close()\n",
        "\n",
        "        logging.info('Data loaded into Postgres database')\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f'Error loading data into Postgres database: {e}')\n",
        "        raise\n",
        "\n",
        "\n",
        "with dag:\n",
        "\n",
        "    extract_data_task = PythonOperator(\n",
        "        task_id='extract_data_id1',\n",
        "        python_callable=extract_data\n",
        "    )\n",
        "\n",
        "    transform_data_task = PythonOperator(\n",
        "        task_id='transform_data',\n",
        "        python_callable=transform_data\n",
        "    )\n",
        "\n",
        "    load_data_task = PythonOperator(\n",
        "        task_id='load_data',\n",
        "        python_callable=load_data\n",
        "    )\n",
        "\n",
        "    # define dependencies\n",
        "    extract_data_task >> transform_data_task >> load_data_task\n"
      ],
      "metadata": {
        "id": "CCpiyyv6IZRN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Documentation of the pipeline**:\n",
        "\n",
        "The data pipeline consists of three tasks:\n",
        "\n",
        "**extract_data_task**: reads in CSV files containing customer, order, and payment data and returns them as Pandas dataframes.\n",
        "**transform_data_task**: takes the three dataframes as inputs and performs data cleaning and transformation operations, such as merging, grouping, and aggregating, to compute the customer lifetime value for each customer.\n",
        "**load_data_task**: takes the transformed data as input and loads it into a Postgres database. The data is inserted into a table named \"subscriber_data\".\n",
        "The three tasks are defined as PythonOperator instances in an Airflow DAG named \"data_pipeline\". The DAG is scheduled to run daily.\n",
        "\n",
        "Best practices:\n",
        "\n",
        "**Modularization**: The pipeline is split into three tasks, each responsible for a specific part of the data processing. This approach makes the code easier to understand, test, and maintain.\n",
        "\n",
        "**Error handling**: The load_data_task is wrapped in a try/except block, which catches any exceptions that may occur during the loading process and logs an error message. This practice makes the pipeline more resilient to errors and helps to prevent data loss.\n",
        "\n",
        "**Logging**: The load_data_task logs its progress and any errors that occur during the data loading process. This practice makes it easier to debug the pipeline and monitor its performance.\n",
        "\n",
        "**Recommendations for deployment and running the pipeline in a cloud-based provider:**\n",
        "**Use a managed service**: Consider using a managed service like Amazon Redshift or Google BigQuery for your data storage and processing needs. These services are highly scalable, fault-tolerant, and can handle large volumes of data. They also have built-in security and data governance features.\n",
        "\n",
        "**Containerize your application**: Use containers to package your application and its dependencies so that it can be easily deployed on any cloud-based provider. This will allow you to move your application between different environments without worrying about compatibility issues.\n",
        "\n",
        "**Use a managed workflow service**: Use a managed workflow service like AWS Step Functions or Google Cloud Composer to manage the execution of your data pipeline. These services provide a visual interface for building, scheduling, and monitoring workflows, and can automatically retry failed tasks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2L-jf6ORPAf1"
      }
    }
  ]
}